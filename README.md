# SMPR
SMPR Language R
# Метрические алгоритмы классификации
  В задачах классификации схожие объекты
гораздо чаще лежат в одном классе, чем в разных. Если задача поддаётся
решению, то граница между классами не может «проходить повсюду»; классы
образуют компактно локализованные подмножества в пространстве объектов. Это
предположение принято называть гипотезой компактности1.

  Имеется пространство объектов X и конечное множество имён классов Y .
На множестве X задана функция расстояния ρ: X ×X → [0,∞). Существует целевая
зависимость y*: X → Y , значения которой известны только на объектах обучающей
выборки ![](https://github.com/Ismailodabashi/SMPR/blob/master/CodeCogsEqn.gif), yi = y*(xi). Требуется построить алгоритм классификации
a: X → Y , аппроксимирующий целевую зависимость y*(x) на всём множестве X.

## Метод ближайших соседей
  
**Алгоритм ближайшего соседа** (nearest neighbor, NN) является самым простым
алгоритмом классификации. Он относит классифицируемый объект u ∈ Xℓ к тому
классу, которому принадлежит ближайший обучающий объект:

![](https://github.com/Ismailodabashi/SMPR/blob/master/2.gif)
       
Обучение NN сводится к запоминанию выборки Xℓ.

Для примера использования методов классификация была взята выборка ирисов фишера по длине и ширине лепестка.

![](https://github.com/Ismailodabashi/SMPR/blob/master/Ирисы%20Фишера.png)

**Алгоритм k ближайших соседей** (k nearest neighbors, kNN). Чтобы сгладить
влияние шумовых выбросов, будем классифицировать объекты путём голосования
по k ближайшим соседям. Каждый из соседей голосует за отнесение
объекта u к своему классу. Алгоритм относит объект u к тому классу, который
наберёт большее число голосов:

![](https://github.com/Ismailodabashi/SMPR/blob/master/3.gif)

``` R
## Применяем метод kNN
kNN <- function(xl, z, k)
{
  ## Сортируем выборку согласно классифицируемого объекта
	orderedXl <- sortObjectsByDist(xl, z)
	n <- dim(orderedXl)[2] - 1

  ## Получаем классы первых k соседей
	classes <- orderedXl[1:k, n + 1]

  ##Составляем таблицу встречаемости каждого класса
	counts <- table(classes)

  ## Находим класс, который доминирует среди первых k соседей
	class <- names(which.max(counts))
	return (class)
} 
```

Например для k=3 классифицируем объект с координатами (2.5, 1.5).

![](https://github.com/Ismailodabashi/SMPR/blob/master/k%3D3.png)

### Преимущества:
1. Простота реализации.

### Недостатки:
1. Нужно хранить всю выборку.
2. При *k = 1* неустойчивость к погрешностям (*выбросам* -- объектам, которые окружены объектами чужого класса), вследствие чего этот выброс классифицировался неверно и окружающие его объекты, для которого он окажется ближайшим, тоже.
2. При *k = l* алгоритм наоборот чрезмерно устойчив и вырождается в константу.
3. Крайне бедный набор параметров.
4. Точки, расстояние между которыми одинаково, не все будут учитываться.

## LOO

Оптимальное значение
параметра k определяют по критерию скользящего контроля с исключением
объектов по одному (leave-one-out, LOO). Для каждого объекта xi ∈ Xℓ проверяется,
правильно ли он классифицируется по своим k ближайшим соседям.

![](https://github.com/Ismailodabashi/SMPR/blob/master/4.gif)

``` R
Loo <- function(k,xl)
{
	sum <- 0
	for(i in 1:dim(xl)[1]){
		if(i==1){
				tmpXL <- xl[2:dim(xl)[1],]
			}
			else if (i==dim(xl)[1]) {
				tmpXL <- xl[1:dim(xl)[1]-1,]
			}
			else {
					
				tmpXL <- rbind(xl[1:i-1, ], xl[i+1:dim(xl)[1],])
			}

		xi <- c(xl[i,1], xl[i,2])
		class <-kNN(tmpXL,xi,k)
		if(class != xl[i,3])
		sum <- sum+1
	}
	return(sum)
}

```

Для нашей задачи оптимально k=6, при этом значении алгоритм LOO показывает минимальное колтчество ошибок, по сравнению с другими значениями k.

![](https://github.com/Ismailodabashi/SMPR/blob/master/LOO.jpg)

![](https://github.com/Ismailodabashi/SMPR/blob/master/Loo%20l.png)





