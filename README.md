# SMPR
SMPR Language R
# Метрические алгоритмы классификации
  Алгоритм классификации, основанный на вычислении оценок сходства между объектами. Простейшим метрическим классификатором является метод ближайших соседей, в котором классифицируемый объект относится к тому классу, которому принадлежит большинство схожих с ним объектов.

## Метод ближайших соседей
  
**Алгоритм ближайшего соседа** (nearest neighbor, NN) является самым простым
алгоритмом классификации. Он относит классифицируемый объект u ∈ Xℓ к тому
классу, которому принадлежит ближайший обучающий объект:

![](https://github.com/Ismailodabashi/SMPR/blob/master/2.gif)
       
Обучение NN сводится к запоминанию выборки Xℓ.

Для примера использования методов классификация была взята выборка ирисов фишера по длине и ширине лепестка.

![](https://github.com/Ismailodabashi/SMPR/blob/master/Ирисы%20Фишера.png)

**Алгоритм k ближайших соседей** (k nearest neighbors, kNN). Метрический алгоритм для автоматической классификации объектов.
Объект присваивается к тому классу, который является наиболее распространённым среди k соседей данного элемента, классы которых уже известны.


``` R
## Применяем метод kNN
kNN <- function(xl, z, k)
{
  ## Сортируем выборку согласно классифицируемого объекта
	orderedXl <- sortObjectsByDist(xl, z)
	n <- dim(orderedXl)[2] - 1

  ## Получаем классы первых k соседей
	classes <- orderedXl[1:k, n + 1]

  ##Составляем таблицу встречаемости каждого класса
	counts <- table(classes)

  ## Находим класс, который доминирует среди первых k соседей
	class <- names(which.max(counts))
	return (class)
} 
```

Например для k=3 классифицируем объект с координатами (2.5, 1.5).

![](https://github.com/Ismailodabashi/SMPR/blob/master/k%3D3.png)

### Преимущества:
1. Простота реализации.

### Недостатки:
1. Нужно хранить всю выборку.
2. При *k = 1* неустойчивость к погрешностям (*выбросам* -- объектам, которые окружены объектами чужого класса), вследствие чего этот выброс классифицировался неверно и окружающие его объекты, для которого он окажется ближайшим, тоже.
2. При *k = l* алгоритм наоборот чрезмерно устойчив и вырождается в константу.
3. Крайне бедный набор параметров.
4. Точки, расстояние между которыми одинаково, не все будут учитываться.

## LOO

При k=1 алгоритм ближайшего соседа неустойчив к шумовым выбросам: он даёт ошибочные классификации не только на самих объектах-выбросах, но и на ближайших к ним объектах других классов. При k=m, наоборот, алгоритм чрезмерно устойчив и вырождается в константу. Таким образом, крайние значения k нежелательны. На практике оптимальное значение параметра k определяют по критерию скользящего контроля, чаще всего — методом исключения объектов по одному (leave-one-out cross-validation).

``` R
Loo <- function(k,xl)
{
	sum <- 0
	for(i in 1:dim(xl)[1]){
		if(i==1){
				tmpXL <- xl[2:dim(xl)[1],]
			}
			else if (i==dim(xl)[1]) {
				tmpXL <- xl[1:dim(xl)[1]-1,]
			}
			else {
					
				tmpXL <- rbind(xl[1:i-1, ], xl[i+1:dim(xl)[1],])
			}

		xi <- c(xl[i,1], xl[i,2])
		class <-kNN(tmpXL,xi,k)
		if(class != xl[i,3])
		sum <- sum+1
	}
	return(sum)
}

```

Для нашей задачи оптимально k=6, при этом значении алгоритм LOO показывает минимальное колтчество ошибок, по сравнению с другими значениями k.

![](https://github.com/Ismailodabashi/SMPR/blob/master/LOO.jpg)

![](https://github.com/Ismailodabashi/SMPR/blob/master/График%20зависимости%20LOO(k).png)





